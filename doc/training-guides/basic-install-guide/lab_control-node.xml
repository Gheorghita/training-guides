<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="lab_control-node.xml">
  <title>Controller node</title>
  <para><emphasis role="bold">Network Diagram:</emphasis></para>
  <figure>
    <title>Network Diagram</title>
    <mediaobject>
      <imageobject>
        <imagedata fileref="../figures/lab_virtual-box/image03.png"
                   contentwidth="7in"/>
      </imageobject>
    </mediaobject>
  </figure>
  <para>Publicly editable image source at <link
    xlink:href="https://docs.google.com/drawings/d/1GX3FXmkz3c_tUDpZXUVMpyIxicWuHs5fNsHvYNjwNNk/edit?usp=sharing"
    >https://docs.google.com/drawings/d/1GX3FXmkz3c_tUDpZXUVMpyIxicWuHs5fNsHvYNjwNNk/edit?usp=sharing</link></para>
  <para><emphasis role="bold">Vboxnet0</emphasis>, <emphasis
      role="bold">Vboxnet1</emphasis>, <emphasis role="bold"
      >Vboxnet2</emphasis> - are virtual networks setup up by virtual
    box with your host machine. This is the way your host can
    communicate with the virtual machines. These networks are in turn
    used by VirtualBox VMs for OpenStack networks, so
    that OpenStack’s services can communicate with each other.</para>
  <note>
      <para>On reboot the node VM may lose internet and network connectivity.
          Restart the networking service and use the <command>ping</command>
          command to verify the network connectivity for the given VM.</para>
  </note>
  <note>
    <para>To avoid issues on the VirtualBox virtual machine
      (controller node), <emphasis role="bold">save the virtual
        machine</emphasis> state instead of completing a reboot or
      shut down.</para>
  </note>
  <note>
    <para>Take regular snapshots of the VirtualBox virtual machines after
          each section. In case the VM is broken, you may revert back to the
          snapshot to save time and effort.</para>
  </note>
  <para><guilabel>Controller node</guilabel></para>
  <para>
      Start the controller node which was set up in a previous section.
  </para>
  <para><emphasis role="bold">Preparing Ubuntu 14.04</emphasis></para>
  <para><emphasis role="bold">Networking :</emphasis></para>
  <para>Configure your network by editing the
      <filename>/etc/network/interfaces</filename> file</para>
  <itemizedlist>
    <listitem>
      <para>Open <filename>/etc/network/interfaces</filename> and edit the
        file as mentioned:</para>
    <programlisting>
# This file is for the OpenStack controller node for OpenStack training project.
# Note: Selection of the IP addresses is important.
# Any changes to the IP addresses may break OpenStack related services.

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface - VirtualBox NAT connection
# (VirtualBox Network Adapter 1)
auto eth0
iface eth0 inet dhcp

# VirtualBox vboxnet0 - OpenStack management network
# (VirtualBox Network Adapter 2)
auto eth1
iface eth1 inet static
address 10.10.10.51
netmask 255.255.255.0

# VirtualBox vboxnet2 - OpenStack API network
# (VirtualBox Network Adapter 3)
auto eth2
iface eth2 inet static
address 192.168.100.51
netmask 255.255.255.0
</programlisting>
    </listitem>
    <listitem>
      <para>After saving the interfaces file, restart the networking
        service:</para>
      <screen><prompt>#</prompt> <userinput>service networking restart</userinput></screen>
      <screen><prompt>#</prompt> <userinput>ifconfig</userinput></screen>
    </listitem>
    <listitem>
        <para>Verify if the network interfaces have the given IP addresses as
        configured above.</para>
    </listitem>
  </itemizedlist>
  <para><emphasis role="bold">SSH from host</emphasis></para>
  <itemizedlist>
    <listitem>
      <para>To SSH into the controller node from the host machine, type
        the following command.</para>
      <screen><prompt>$</prompt> <userinput>ssh control@10.10.10.51</userinput></screen>
      <screen><prompt>$</prompt> <userinput>sudo su</userinput></screen>
    </listitem>
    <listitem>
      <para>Now you can have access to your host clipboard.</para>
    </listitem>
</itemizedlist>
<para><emphasis role="bold">Update package lists and repository information</emphasis></para>
    <screen>
        <prompt>#</prompt><userinput>apt-get update</userinput>
    </screen>
<para><emphasis role="bold">Update Ubuntu Cloud Archive for Icehouse</emphasis>
</para>
    <para>The <link
      xlink:href="https://wiki.ubuntu.com/ServerTeam/CloudArchive"
      >Ubuntu Cloud Archive</link> is a special repository that
      allows you to install newer releases of OpenStack on the
      stable supported version of Ubuntu.</para>
     <para>Install the Ubuntu Cloud Archive for Icehouse
        <screen>
            <prompt>#</prompt><userinput>apt-get install ubuntu-cloud-keyring software-properties-common \
                python-software-properties</userinput>
            <prompt>#</prompt><userinput>add-apt-repository cloud-archive:icehouse</userinput>
        </screen>
    </para>
    <para>Update the package database and upgrade your system:</para>
    <screen>
    <prompt>#</prompt> <userinput>apt-get update</userinput>
    <prompt>#</prompt> <userinput>apt-get dist-upgrade</userinput>
    </screen>
    <para>Reboot the system for all changes to take effect:</para>
    <screen>
        <prompt>#</prompt> <userinput>reboot</userinput>
    </screen>
    <para>Install vlan and bridge-utils packages:</para>
    <para>
        <screen><prompt>#</prompt> <userinput>apt-get install vlan bridge-utils</userinput></screen>
    </para>
    <para>Install NTP:</para>
    <para>
        <screen><prompt>#</prompt> <userinput>apt-get install -y ntp</userinput></screen>
    </para>
    <para>Configure NTP Server to controller node:</para>
    <para>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 0.ubuntu.pool.ntp.org/#server 0.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 1.ubuntu.pool.ntp.org/#server 1.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 2.ubuntu.pool.ntp.org/#server 2.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 3.ubuntu.pool.ntp.org/#server 3.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server ntp.ubuntu.com/server 10.10.10.51/g'/etc/ntp.conf</userinput></screen>
    </para>
  <para><emphasis role="bold">MySQL</emphasis></para>
  <itemizedlist>
    <listitem>
      <para>Install MySQL:</para>
      <screen>
          <prompt>#</prompt>
          <userinput>apt-get install -y mysql-server python-mysqldb</userinput>
      </screen>
    </listitem>
    <listitem>
      <para>Configure mysql to accept all incoming requests:</para>
      <screen>
          <prompt>#</prompt> <userinput>sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf</userinput>
      </screen>
      <screen>
          <prompt>#</prompt> <userinput>service mysql restart</userinput></screen>
    </listitem>
  </itemizedlist>
  <para><emphasis role="bold">RabbitMQ</emphasis></para>
  <itemizedlist>
    <listitem>
      <para>Install RabbitMQ:</para>
      <screen>
          <prompt>#</prompt>
          <userinput>apt-get install -y rabbitmq-server</userinput>
      </screen>
    </listitem>
    <listitem>
      <para>Create these databases:</para>
      <screen><prompt>$</prompt> <userinput>mysql -u root -p</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>CREATE DATABASE keystone;</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>GRANT ALL ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'KEYSTONE_DBPASS';</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>CREATE DATABASE glance;</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>GRANT ALL ON glance.* TO 'glance'@'%' IDENTIFIED BY 'GLANCE_DBPASS';</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>CREATE DATABASE neutron;</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>GRANT ALL ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'NEUTRON_DBPASS';</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>CREATE DATABASE nova;</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>GRANT ALL ON nova.* TO 'nova'@'%' IDENTIFIED BY 'NOVA_DBPASS';</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>CREATE DATABASE cinder;</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>GRANT ALL ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'CINDER_DBPASS';</userinput></screen>
      <screen><prompt>mysql></prompt> <userinput>quit;</userinput></screen>
    </listitem>
  </itemizedlist>
  <para><emphasis role="bold">Installing Keystone</emphasis></para>
  <itemizedlist>
    <listitem>
      <para>Install the Keystone packages:</para>
      <screen><prompt>#</prompt> <userinput>apt-get install -y keystone</userinput></screen>
    </listitem>
    <listitem>
      <para>Adapt the connection attribute in the
          <filename>/etc/keystone/keystone.conf</filename> to the new
        database:</para>
      <programlisting>connection = mysql://keystone:KEYSTONE_DBPASS@10.10.10.51/keystone</programlisting>
    </listitem>
    <listitem>
      <para>Restart the identity service then synchronize the
        database:</para>
      <screen><prompt>#</prompt> <userinput>service keystone restart</userinput></screen>
      <screen><prompt>#</prompt> <userinput>keystone-manage db_sync</userinput></screen>
    </listitem>
    <listitem>
      <para>Fill up the keystone database using the following
        scripts:</para>
      <para>
        <link
            xlink:href="http://git.openstack.org/cgit/openstack/training-guides/plain/doc/training-guides/basic-install-guide/keystone-scripts/keystone_basic.sh">
          <filename>keystone_basic.sh</filename>
        </link></para>
      <para>
        <link
            xlink:href="http://git.openstack.org/cgit/openstack/training-guides/plain/doc/training-guides/basic-install-guide/keystone-scripts/keystone_endpoints_basic.sh">
          <filename>keystone_endpoints_basic.sh</filename>
        </link></para>
    </listitem>
    <listitem>
      <para>Run scripts:</para>
      <screen><prompt>$</prompt> <userinput>chmod +x keystone_basic.sh</userinput></screen>
      <screen><prompt>$</prompt> <userinput>chmod +x keystone_endpoints_basic.sh</userinput></screen>
      <screen><prompt>$</prompt> <userinput>./keystone_basic.sh</userinput></screen>
      <screen><prompt>$</prompt> <userinput>./keystone_endpoints_basic.sh</userinput></screen>
    </listitem>
    <listitem>
      <para>Create a simple credentials file</para>
      <programlisting>nano Credentials.sh</programlisting>
    </listitem>
    <listitem>
      <para>Paste the following:</para>
      <screen><prompt>$</prompt> <userinput>export OS_TENANT_NAME=admin</userinput></screen>
      <screen><prompt>$</prompt> <userinput>export OS_USERNAME=admin</userinput></screen>
      <screen><prompt>$</prompt> <userinput>export OS_PASSWORD=admin_pass</userinput></screen>
      <screen><prompt>$</prompt> <userinput>export OS_AUTH_URL="http://192.168.100.51:5000/v2.0/"</userinput></screen>
    </listitem>
    <listitem>
      <para>Load the above credentials:</para>
      <screen><prompt>$</prompt> <userinput>source Credentials.sh</userinput></screen>
    </listitem>
    <listitem>
      <para>To test Keystone, we use a simple CLI command:</para>
      <screen><prompt>$</prompt> <userinput>keystone user-list</userinput></screen>
    </listitem>
  </itemizedlist>
  <para><emphasis role="bold">Glance</emphasis></para>
  <para>The OpenStack Glance project provides services for
    discovering, registering, and retrieving virtual machine images.
    Glance has a RESTful API that allows querying of VM image metadata
    as well as retrieval of the actual image.</para>
  <para>VM images made available through Glance can be stored in a
    variety of locations from simple file systems to object-storage
    systems like the OpenStack Swift project.</para>
  <para>Glance, as with all OpenStack projects, is written with the
    following design guidelines in mind:</para>
  <itemizedlist>
    <listitem>
      <para>Component based architecture: Quickly adds new
        behaviors</para>
    </listitem>
    <listitem>
      <para>Highly available: Scales to very serious workloads</para>
    </listitem>
    <listitem>
      <para>Fault tolerant: Isolated processes avoid cascading
        failures</para>
    </listitem>
    <listitem>
      <para>Recoverable: Failures should be easy to diagnose, debug,
        and rectify</para>
    </listitem>
    <listitem>
      <para>Open standards: Be a reference implementation for a
        community-driven api</para>
    </listitem>
    <listitem>
      <para>Install Glance:</para>
      <screen><prompt>#</prompt> <userinput>apt-get install -y glance</userinput></screen>
    </listitem>
    <listitem>
      <para>Update
          <filename>/etc/glance/glance-api-paste.ini</filename>:</para>
      <programlisting>[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
delay_auth_decision = true
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = glance
admin_password = service_pass</programlisting>
    </listitem>
    <listitem>
      <para>Update the
          <filename>/etc/glance/glance-registry-paste.ini</filename>:</para>
      <programlisting>[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = glance
admin_password = service_pass</programlisting>
    </listitem>
    <listitem>
      <para>Update the
          <filename>/etc/glance/glance-api.conf</filename>:</para>
      <programlisting>
[DEFAULT]
rpc_backend = rabbit
rabbit_host = 10.10.10.51

[database]
sql_connection = mysql://glance:GLANCE_DBPASS@10.10.10.51/glance
[keystone_authtoken]
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = glance
admin_password = service_pass

[paste_deploy]
flavor = keystone</programlisting>
    </listitem>
    <listitem>
      <para>Update the
          <filename>/etc/glance/glance-registry.conf</filename>:</para>
      <programlisting>[database]
sql_connection = mysql://glance:GLANCE_DBPASS@10.10.10.51/glance
[keystone_authtoken]
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = glance
admin_password = service_pass

[paste_deploy]
flavor = keystone</programlisting>
    </listitem>
    <listitem>
      <para>Restart the glance-api and glance-registry
        services:</para>
      <screen><prompt>#</prompt> <userinput>service glance-api restart; service glance-registry restart</userinput></screen>
    </listitem>
    <listitem>
      <para>Synchronize the Glance database:</para>
      <screen><prompt>#</prompt> <userinput>glance-manage db_sync</userinput></screen>
    </listitem>
    <listitem>
      <para>To test Glance, upload the “cirros cloud image” directly
        from the internet:</para>
      <screen><prompt>$</prompt> <userinput>glance image-create --name OS4Y_Cirros --is-public true --container-format bare --disk-format qcow2 --location https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img</userinput></screen>
    </listitem>
    <listitem>
      <para>Check if the image is successfully uploaded:</para>
      <screen><prompt>$</prompt> <userinput>glance image-list</userinput></screen>
    </listitem>
  </itemizedlist>
  <para><emphasis role="bold">Nova</emphasis></para>
  <para>Nova is the project name for OpenStack Compute, a cloud
    computing fabric controller, the main part of an IaaS system.
    Individuals and organizations can use Nova to host and manage
    their own cloud computing systems. Nova originated as a project
    out of NASA Ames Research Laboratory.</para>
  <para>Nova is written with the following design guidelines in
    mind:</para>
      <para>Install nova components:</para>
      <screen>
        <prompt>#</prompt> <userinput>apt-get install -y nova-api nova-cert nova-conductor nova-consoleauth nova-novncproxy nova-scheduler python-novaclient</userinput>
      </screen>
<itemizedlist>
    <listitem>
      <para>Edit <filename>/etc/nova/nova.conf</filename></para>
      <programlisting>[database]
connection = mysql://nova:NOVA_DBPASS@10.10.10.51/nova

[DEFAULT]
rpc_backend = rabbit
rabbit_host = 10.10.10.51
my_ip = 10.10.10.51
vncserver_listen = 10.10.10.51
vncserver_proxyclient_address = 10.10.10.51
auth_strategy = keystone

network_api_class = nova.network.neutronv2.api.API
neutron_url = http://10.10.10.51:9696
neutron_auth_strategy = keystone
neutron_admin_tenant_name = service
neutron_admin_username = neutron
neutron_admin_password = service_pass
neutron_admin_auth_url = http://10.10.10.51:35357/v2.0
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
security_group_api = neutron
service_neutron_metadata_proxy = true
neutron_metadata_proxy_shared_secret = OpenStackTraining

[keystone_authtoken]
auth_uri = http://10.10.10.51:5000
auth_host = controller
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = nova
admin_password = service_pass
      </programlisting>
    </listitem>
    <listitem>
      <para>Synchronize your database:</para>
      <screen><prompt>#</prompt> <userinput>nova-manage db sync</userinput></screen>
    </listitem>
    <listitem>
      <para>Restart nova-* services (all nova services):</para>
      <screen><prompt>#</prompt><userinput>service nova-api restart</userinput></screen>
      <screen><prompt>#</prompt><userinput>service nova-cert restart</userinput></screen>
      <screen><prompt>#</prompt><userinput>service nova-consoleauth restart</userinput></screen>
      <screen><prompt>#</prompt><userinput>service nova-scheduler restart</userinput></screen>
      <screen><prompt>#</prompt><userinput>service nova-conductor restart</userinput></screen>
      <screen><prompt>#</prompt><userinput>service nova-novncproxy restart</userinput></screen>
    </listitem>
    <listitem>
        <para>Check for the smiling faces on
            <systemitem class="service">nova-*</systemitem> services to confirm your
        installation:</para>
      <screen><prompt>#</prompt> <userinput>nova-manage service list</userinput></screen>
    </listitem>
  </itemizedlist>
<para><emphasis role="bold">Neutron</emphasis></para>
  <para>Neutron is an OpenStack project to provide “network
    connectivity as a service" between interface devices (e.g., vNICs)
    managed by other OpenStack services (e.g., nova).</para>
  <itemizedlist>
    <listitem>
      <para>Install the Neutron Server and the Open vSwitch package
        collection:</para>
      <screen><prompt>#</prompt> <userinput>apt-get install -y neutron-server neutron-plugin-ml2</userinput></screen>
    </listitem>
    <listitem>
      <para>Edit the
          <filename>/etc/neutron/plugins/ml2/ml2_conf.ini</filename>:</para>
      <programlisting>[ml2]
type_drivers = gre
tenant_network_types = gre
mechanism_drivers = openvswitch

[ml2_type_gre]
tunnel_id_ranges = 1:1000

[securitygroup]
firewall_driver =
neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
enable_security_group = True</programlisting>
    </listitem>
    <listitem>
      <para>Edit the
        <filename>/etc/neutron/api-paste.ini</filename>:</para>
      <programlisting>[filter:authtoken]
firewall_driver =
neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverpaste.
filter_factory = keystoneclient.middleware.auth_token:filter_factory
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = neutron
admin_password = service_pass</programlisting>
    </listitem>
    <listitem>
      <para>Edit the
        <filename>/etc/neutron/neutron.conf</filename>:</para>
      <programlisting>[DEFAULT]
auth_strategy = keystone
rpc_backend = neutron.openstack.common.rpc.impl_kombu
rabbit_host = controller
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
nova_url = http://controller:8774/v2
nova_admin_username = nova
nova_admin_tenant_id = SERVICE_TENANT_ID
nova_admin_password = NOVA_PASS
nova_admin_auth_url = http://controller:35357/v2.0

[keystone_authtoken]
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = neutron
admin_password = service_pass
signing_dir = /var/lib/neutron/keystone-signing

[database]
connection = mysql://neutron:NEUTRON_DBPASS@10.10.10.51/neutron</programlisting>
    </listitem>
    <listitem>
      <para>Restart Nova Services</para>
      <screen><prompt>#</prompt> <userinput>service nova-api restart</userinput></screen>
      <screen><prompt>#</prompt> <userinput>service nova-scheduler restart</userinput></screen>
      <screen><prompt>#</prompt> <userinput>service nova-conductor restart</userinput></screen>
      <para>Restart Neutron services:</para>
      <screen><prompt>#</prompt> <userinput>service neutron-server restart</userinput></screen>
    </listitem>
  </itemizedlist>

  <para><emphasis role="bold">Cinder</emphasis></para>
  <para>Cinder is an OpenStack project that provides “block storage as a
    service”.</para>
  <itemizedlist>
    <listitem>
      <para>Install Cinder components:</para>
      <screen><prompt>#</prompt> <userinput>apt-get install -y cinder-api cinder-scheduler cinder-volume lvm2</userinput></screen>
    </listitem>
    <listitem>
      <para>Edit <filename>/etc/cinder/cinder.conf</filename>:</para>
      <programlisting>[database]
sql_connection = mysql://cinder:CINDER_DBPASS@10.10.10.51/cinder

[keystone_authtoken]
auth_uri = http://10.10.10.51:5000
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = cinder
admin_password = service_pass

[DEFAULT]
rpc_backend = cinder.openstack.common.rpc.impl_kombu
rabbit_host = 10.10.10.51
rabbit_port = 5672
rabbit_userid = guest
</programlisting>
    </listitem>
    <listitem>
      <para>Then, synchronize Cinder database:</para>
      <screen><prompt>#</prompt> <userinput>cinder-manage db sync</userinput></screen>
    </listitem>
    <listitem>
      <para>Restart Cinder Services</para>
      <screen><prompt>#</prompt> <userinput>service cinder-scheduler restart</userinput></screen>
      <screen><prompt>#</prompt> <userinput>service cinder-api restart</userinput></screen>
    </listitem>
    <listitem>
      <para>Finally, create a volume group and name it
          <literal>cinder-volumes</literal>:</para>
      <screen><prompt>#</prompt> <userinput>dd if=/dev/zero of=cinder-volumes bs=1 count=0 seek=2G</userinput></screen>
      <screen><prompt>#</prompt> <userinput>losetup /dev/loop2 cinder-volumes</userinput></screen>
      <screen><prompt>#</prompt> <userinput>fdisk /dev/loop2</userinput></screen>
      <screen><prompt>Command (m for help):</prompt> <userinput>n</userinput></screen>
      <screen><prompt>Command (m for help):</prompt> <userinput>p</userinput></screen>
      <screen><prompt>Command (m for help):</prompt> <userinput>1</userinput></screen>
      <screen><prompt>Command (m for help):</prompt> <userinput>t</userinput></screen>
      <screen><prompt>Command (m for help):</prompt> <userinput>8e</userinput></screen>
      <screen><prompt>Command (m for help):</prompt> <userinput>w</userinput></screen>
    </listitem>
    <listitem>
      <para>Proceed to create the physical volume then the volume
        group:</para>
      <screen><prompt>#</prompt> <userinput>pvcreate /dev/loop2</userinput></screen>
      <screen><prompt>#</prompt> <userinput>vgcreate cinder-volumes /dev/loop2</userinput></screen>
    </listitem>
    <listitem>
      <note>
        <para>Be aware that this volume group gets lost after a system
          reboot. If you do not want to perform this step again, make
          sure that you save the machine state and do not shut it
          down.</para>
      </note>
    </listitem>
  </itemizedlist>
  <para><emphasis role="bold">Horizon</emphasis></para>
  <para>Horizon is the canonical implementation of OpenStack’s
    dashboard, which provides a web-based user interface to OpenStack
    services including Nova, Swift, Keystone, etc.</para>
  <itemizedlist>
    <listitem>
      <para>To install Horizon, complete these steps:</para>
      <screen><prompt>#</prompt> <userinput>apt-get install -y openstack-dashboard memcached</userinput></screen>
    </listitem>
    <listitem>
      <para>If you do not like the OpenStack Ubuntu Theme, you can
        remove it with the below command:</para>
      <screen><prompt>#</prompt> <userinput>dpkg --purge openstack-dashboard-ubuntu-theme</userinput></screen>
    </listitem>
    <listitem>
      <para>Reload Apache and memcached:</para>
      <screen><prompt>#</prompt> <userinput>service apache2 restart; service memcached restart</userinput></screen>
    </listitem>
  </itemizedlist>
</chapter>
