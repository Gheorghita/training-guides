<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink"
 version="5.0"
  xml:id="lab_compute-node">
  <title>Compute Node</title>
  <orderedlist>
    <listitem>
      <para><emphasis role="bold">Network Diagram:</emphasis></para>
    </listitem>
  </orderedlist>
  <figure>
    <title>Network Diagram</title>
    <mediaobject>
      <imageobject>
        <imagedata fileref="../figures/lab_virtual-box/image03.png"
                   contentwidth="7in"/>
      </imageobject>
    </mediaobject>
  </figure>
  <para>Publicly editable image source at <link
    xlink:href="https://docs.google.com/drawings/d/1GX3FXmkz3c_tUDpZXUVMpyIxicWuHs5fNsHvYNjwNNk/edit?usp=sharing"
    >https://docs.google.com/drawings/d/1GX3FXmkz3c_tUDpZXUVMpyIxicWuHs5fNsHvYNjwNNk/edit?usp=sharing</link></para>
  <para><emphasis role="bold">Vboxnet0</emphasis>, <emphasis
      role="bold">Vboxnet1</emphasis>, <emphasis role="bold"
      >Vboxnet2</emphasis> - are virtual networks set up up by VirtualBox
    with your host machine. This is the way the host can
    communicate with the virtual machines. These networks are in turn
    used by VirtualBox VMs for OpenStack networks, so that
    OpenStackâ€™s services can communicate with each other.</para>
  <para><guilabel>Compute node</guilabel></para>
  <para>Start the controller node, which was set up in a previous section.</para>
  <note>
      <para>After the reboot of the node, the VM may lose internet and network
          connectivity. Restart the networking service and use the
          <command>ping</command> command to verify the network
          connectivity for the given VM.</para>
  </note>
  <note>
      <para>Take regular snapshots of the VirtualBox virtual machines after
          each section. In case the VM is broken, you may revert back to the
          snapshot to save time and effort.</para>
  </note>
  <para><guilabel>Controller node</guilabel></para>
  <para>
      Start the controller node, which was set up in a previous section.
  </para>
  <para><emphasis role="bold">Preparing Ubuntu 14.04</emphasis></para>
  <para><emphasis role="bold">Networking:</emphasis></para>
  <para>Configure your network by editing the
      <filename>/etc/network/interfaces</filename> file</para>
  <itemizedlist>
    <listitem>
      <para>Open <filename>/etc/network/interfaces</filename> and edit the
        file as mentioned:</para>
    <programlisting>
# This file is for the OpenStack compute node for the OpenStack training project.
# Note: Selection of the IP addresses is important.
# Any changes to the IP addresses may break OpenStack related services.

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface - VirtualBox NAT connection
# (VirtualBox Network Adapter 1)
auto eth0
iface eth0 inet dhcp

# VirtualBox vboxnet0 - OpenStack management network
# (VirtualBox Network Adapter 2)
auto eth1
iface eth1 inet static
address 10.10.10.53
netmask 255.255.255.0

# VirtualBox vboxnet2 - OpenStack VM data/communication network
# (VirtualBox Network Adapter 3)
auto eth2
iface eth2 inet static
address 10.20.20.53
netmask 255.255.255.0
</programlisting>
    </listitem>
    <listitem>
      <para>After saving the interfaces file, bring the
        interfaces up:</para>
      <screen><prompt>#</prompt> <userinput>ifup eth1;ifup eth2</userinput></screen>
      <screen><prompt>#</prompt> <userinput>ifconfig</userinput></screen>
    </listitem>
    <listitem>
        <para>The expected network interface should match with the required IP
            addresses as configured above.</para>
    </listitem>
</itemizedlist>
  <para><emphasis role="bold">SSH from host</emphasis></para>
  <itemizedlist>
    <listitem>
        <para>To SSH into the compute node from the host machine, type the
            command below:</para>
      <screen><prompt>$</prompt> <userinput>ssh compute@10.10.10.53</userinput></screen>
      <screen><prompt>$</prompt> <userinput>sudo su</userinput></screen>
    </listitem>
</itemizedlist>

  <para><emphasis role="bold">Preparing Ubuntu 14.04</emphasis></para>
  <itemizedlist>
    <listitem>
      <para>After installing Ubuntu Server, switch to the root user</para>
      <para>
        <screen><prompt>$</prompt> <userinput>sudo su</userinput></screen>
      </para>
    </listitem>
    <listitem>
      <para>Add the Icehouse repositories:</para>
      <para>
        <screen><prompt>#</prompt><userinput>apt-get install ubuntu-cloud-keyring python-software-properties software-properties-common python-keyring</userinput></screen>
        <screen><prompt>#</prompt><userinput>add-apt-repository cloud-archive:icehouse</userinput></screen>
      </para>
    </listitem>
    <listitem>
      <para>Update your system:</para>
      <para>
        <screen><prompt>#</prompt><userinput>apt-get update</userinput></screen>
        <screen><prompt>#</prompt><userinput>apt-get upgrade</userinput></screen>
        <screen><prompt>#</prompt><userinput>apt-get dist-upgrade</userinput></screen>
      </para>
    </listitem>
    <listitem>
        <para>Restart the machine for the changes to apply:</para>
        <screen><prompt>#</prompt> <userinput>reboot</userinput></screen>
    </listitem>
    <listitem>
        <para>Install vlan and bridge-utils packages:</para>
        <screen><prompt>#</prompt> <userinput>apt-get install vlan bridge-utils</userinput></screen>
    </listitem>
    <listitem>
        <para>Install NTP:</para>
    <para>
        <screen><prompt>#</prompt> <userinput>apt-get install ntp</userinput></screen>
    </para>
    </listitem>
    <listitem>
    <para>Configure NTP Server to controller node:</para>
    <para>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 0.ubuntu.pool.ntp.org/#server 0.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 1.ubuntu.pool.ntp.org/#server 1.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 2.ubuntu.pool.ntp.org/#server 2.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server 3.ubuntu.pool.ntp.org/#server 3.ubuntu.pool.ntp.org/g' /etc/ntp.conf</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sed -i 's/server ntp.ubuntu.com/server 10.10.10.51/g'/etc/ntp.conf</userinput></screen>
   </para>
   </listitem>
   <listitem>
      <para>Enable IP forwarding by adding the following to <filename>/etc/sysctl.conf</filename>:</para>
      <para>
        <programlisting>net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0</programlisting>
      </para>
    </listitem>
    <listitem>
      <para>Run the following commands:</para>
      <para>
        <screen><prompt>#</prompt> <userinput>sysctl net.ipv4.ip_forward=1</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sysctl net.ipv4.conf.all.rp_filter=0</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sysctl net.ipv4.conf.default.rp_filter=0</userinput></screen>
        <screen><prompt>#</prompt> <userinput>sysctl -p</userinput></screen>
      </para>
    </listitem>
</itemizedlist>
<para><emphasis role='bold'>Nova and KVM</emphasis></para>
<itemizedlist>
    <listitem>
        <para>Install the Compute packages:</para>
        <screen><prompt>#</prompt><userinput>apt-get install nova-compute-kvm python-guestfs</userinput></screen>
        <screen><prompt>#</prompt><userinput>dpkg-statoverride  --update --add root root 0644 /boot/vmlinuz-$(uname -r)</userinput></screen>
    </listitem>
    <listitem>
        <para>Configure <filename>/etc/nova/nova.conf</filename></para>
        <para><programlisting>[DEFAULT]
auth_strategy = keystone
rpc_backend = rabbit
rabbit_host = 10.10.10.51
my_ip = 10.10.10.53
vnc_enabled = True
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = 10.10.10.53
novncproxy_base_url = http://10.10.10.51:6080/vnc_auto.html
glance_host = 10.10.10.51
network_api_class = nova.network.neutronv2.api.API
neutron_url = http://10.10.10.51:9696
neutron_auth_strategy = keystone
neutron_admin_tenant_name = service
neutron_admin_username = neutron
neutron_admin_password = service_pass
neutron_admin_auth_url = http://10.10.10.51:35357/v2.0
linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
security_group_api = neutron

[database]
# The SQLAlchemy connection string used to connect to the database
connection = mysql://novaUSER:novaPass@10.10.10.51/nova

[keystone_authtoken]
auth_uri = http://10.10.10.51:5000
auth_host = 10.10.10.51
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = nova
admin_password = service_pass</programlisting></para>
    </listitem>
    <listitem>
        <para>Edit <filename>/etc/nova/nova-compute.conf</filename></para>
        <para><programlisting>[libvirt]
virt_type = qemu</programlisting></para>
    </listitem>
    <listitem>
        <para>Restart the Nova Compute Service:</para>
        <screen><prompt>#</prompt><userinput>service nova-compute restart</userinput></screen>
    </listitem>
</itemizedlist>

  <para><emphasis role="bold">Neutron and OVS</emphasis></para>
  <itemizedlist>
    <listitem>
      <para>Install Open vSwitch:</para>
      <screen><prompt>#</prompt> <userinput>apt-get install -y neutron-common neutron-plugin-ml2 neutron-plugin-openvswitch-agent</userinput></screen>
    </listitem>
    <listitem>
        <para>Edit <filename>/etc/neutron/plugins/ml2/ml2_conf.ini</filename></para>
        <para><programlisting>[ml2]
type_drivers = gre
tenant_network_types = gre
mechanism_drivers = openvswitch

[ml2_type_gre]
tunnel_id_ranges = 1:1000

[ovs]
local_ip = 10.20.20.53
tunnel_type = gre
enable_tunneling = True

[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
enable_security_group = True</programlisting></para>
    </listitem>
    <listitem>
      <para>Edit <filename>/etc/neutron/neutron.conf</filename></para>
      <para><programlisting>[DEFAULT]
auth_strategy = keystone
rpc_backend = neutron.openstack.common.rpc.impl_kombu
rabbit_host = 10.10.10.51
rabbit_password = RABBIT_PASS
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True

[keystone_authtoken]
...
auth_uri = http://10.10.10.51:5000
auth_host = 10.10.10.51
auth_protocol = http
auth_port = 35357
admin_tenant_name = service
admin_user = neutron
admin_password = service_pass</programlisting></para>
    </listitem>
    <listitem>
      <para>Restart all the services:</para>
      <para>
        <screen><prompt>#</prompt> <userinput>service openvswitch-switch restart</userinput></screen>
      </para>
    </listitem>
    <listitem>
        <para>Add the integration bridge:</para>
        <screen><prompt>#</prompt><userinput>ovs-vsctl add-br br-int</userinput></screen>
    </listitem>
    <listitem>
  <para><emphasis role="bold">Nova</emphasis></para>
      <para>List nova services (Check for the Smiley Faces to know if the services are running):</para>
      <screen><prompt>#</prompt> <userinput>nova-manage service list</userinput></screen>
    </listitem>
  </itemizedlist>
</chapter>
